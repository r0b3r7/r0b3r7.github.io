<!DOCTYPE html>
<html>
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<link rel="stylesheet" href="assets/main.css" />
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-TBTYV2YN7V"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-TBTYV2YN7V');
</script>
</head>
<body>
<div id="top" >
  <p><a href="/" title="Robert Sidor">-Robert Sidor-</a> <a href="about-me.html" title="about me" class="active">-about me-</a></p>
</div>

<div style="overflow:auto">

  <div class="main">
    <h2>About the team not the metrics (DORA)</h2>
    <p>DORA Metrics, what I wanted to talk about today, is an acronym for "DevOps Research and Assessment". In a nutshell, it's a set of performance indicators (loved by all managers - KPIs) developed to measure the efficiency and quality of teams involved in software implementation, more precisely, the time and quality of delivering solutions to the client. Today, it's hard to imagine "static" programming teams that can't dynamically respond to market and customer needs, whether it's a fix or an expansion of the provided software's functionality. That's why optimizing and adapting this approach is extremely important, and DORA offers specific metrics that primarily help teams assess and refine their practices.</p>
    
    <h4>Origins</h4>
    <p>The metric was created in response to the growing needs of the industry. Its creators realized that traditional methods of evaluating the work of developer teams were not always adequate for new challenges and the intricate world of DevOps. They needed a tool that genuinely reflects work efficiency while also serving as a compass pointing in the direction of improvement.</p>
    <p>The primary goal of DORA was to create a set of metrics that are objective, measurable, and provide specific guidelines on where the team stands in terms of DevOps practices and the direction it should develop. DORA was conceived by experts in the field, from engineers and project managers to technology leaders.</p>
    <p>A key assumption in creating DORA was that it shouldn't just be a dry, theoretical metric. Its creators wanted it to be practical and have a real impact on team activities. Therefore, instead of focusing solely on numerical data, DORA emphasizes the importance of organizational culture, practices, and tools that support high performance in the DevOps area.</p>

    <h4>Metrics</h4>
    <p>DORA focuses on four key metrics that measure different aspects of the software deployment lifecycle. These metrics were carefully selected to provide a comprehensive picture of a team's effectiveness.</p>
    <p><b>Deployment Frequency</b> refers to the number of deployments introduced by the team over a specific period. For instance, imagine a team that initially deployed updates only once a quarter, due to complex processes and probably a lack of automation. However, after adopting selected devops practices, the team significantly increased deployment frequency, thus accelerating value delivery to customers.</p>
    <p>Then there's the <b>Lead Time for Changes</b>, which describes the period from when the code gets approved until its deployment to the client. This time can be used to measure software delivery speed. It aids in understanding the team's cycle time and learning how the increase in demand is handled. The shorter the change lead time, the more efficient the team is at deploying code.</p>
    <p>The <b>Change Failure Rate</b> metric tells us what percentage of deployments fail. Imagine a situation where the failure rate was 10%; this means every tenth deployment ends in error. The goal then is to identify fragile elements of the process and modify them appropriately, reducing this metric's value to a minimum.</p>
    <p>Last but not least, the <b>Mean Time to Restore (MTTR)</b> metric. It refers to the time needed to fix an issue in the event of a failure. Monitoring this process often correlates with the SLAs offered by our applications, meaning the lower the measure's value, the higher customer satisfaction.</p>

    <h4>Reflections</h4>
    <p>Using DORA metrics brings many benefits. They provide teams with an objective and measurable way to assess their DevOps practices. They allow teams to identify areas that need attention and implement specific data-driven changes, not just intuition! However, like everything, DORA also has limitations. Without proper interpretation and understanding of context, these metrics can be misinterpreted. There's also the risk that teams may focus on "boosting" these indicators at the expense of other vital aspects, such as code quality or customer satisfaction.</p>
    <p>DORA metrics are a valuable tool for any tech organization looking to enhance its team's efficiency. However, it's essential to remember that they shouldn't be used as an evaluation or accountability tool. The proper use of DORA metrics should focus on identifying improvement areas, not punishing low indicators. The real value of DORA lies in its ability to indicate improvement paths, leading to better team performance and, ultimately, greater customer satisfaction.</p>

    <p>&nbsp;</p>
    <p>&nbsp;</p>
    <p><i>Sep, 2023</i></p>

  </div> <!-- end of main -->

  <div class="right">
    <h2>_blog posts</h2>
    <a href="competency-management.html" title="" class="smaller active">- Competency Management</a>
    <a href="devops-is-more-than-you-think.html" title="" class="smaller active">- DevOps is more than you think</a>
    <a href="dora.html" title="" class="smaller active">- About the team not the metrics (DORA)</a>
    <a href="space.html" title="" class="smaller active">- About the team not the metrics (SPACE)</a>
    <a href="inner-source.html" title="" class="smaller active">- Inner Source</a>
    
  </div>

</div>

<div id="footer"><p>Â© 2023, Robert Sidor</p></div>

</body>
</html>

